{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structured_Abt-Buy does not exist\n",
      "Dataset textual_DBLP-ACM does not exist\n",
      "Dataset textual_Amazon-Google does not exist\n",
      "Dataset textual_Walmart-Amazon does not exist\n",
      "Dataset textual_DBLP-GoogleScholar does not exist\n",
      "Dataset textual_Fodors-Zagats does not exist\n",
      "Dataset textual_Beer does not exist\n",
      "Dataset textual_iTunes-Amazon does not exist\n",
      "Dataset dirty_Abt-Buy does not exist\n",
      "Dataset dirty_Amazon-Google does not exist\n",
      "Dataset dirty_Fodors-Zagats does not exist\n",
      "Dataset dirty_Beer does not exist\n",
      "Total training samples: 75662\n",
      "\n",
      "Dataset structured_Abt-Buy does not exist\n",
      "Dataset textual_DBLP-ACM does not exist\n",
      "Dataset textual_Amazon-Google does not exist\n",
      "Dataset textual_Walmart-Amazon does not exist\n",
      "Dataset textual_DBLP-GoogleScholar does not exist\n",
      "Dataset textual_Fodors-Zagats does not exist\n",
      "Dataset textual_Beer does not exist\n",
      "Dataset textual_iTunes-Amazon does not exist\n",
      "Dataset dirty_Abt-Buy does not exist\n",
      "Dataset dirty_Amazon-Google does not exist\n",
      "Dataset dirty_Fodors-Zagats does not exist\n",
      "Dataset dirty_Beer does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlhelin/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d1cc0ebdf9413ead0653567d0aec0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t3/20c51ts14q30bycfrjdtpg940000gn/T/ipykernel_21543/2064901071.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n"
     ]
    }
   ],
   "source": [
    "import os, sys, time, torch, pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Append the configuration path\n",
    "sys.path.append(\"..\")\n",
    "import config\n",
    "\n",
    "# Load configuration\n",
    "folders = [config.STRUCTURED_DIR, config.TEXTUAL_DIR, config.DIRTY_DIR]\n",
    "datasets = [\n",
    "    config.DBLP_ACM_DIR, config.ABT_BUY_DIR, config.AMAZON_GOOGLE_DIR,\n",
    "    config.WALMART_AMAZON_DIR, config.DBLP_GOOGLESCHOLAR_DIR,\n",
    "    config.FODORS_ZAGATS_DIR, config.BEER_DIR, config.ITUNES_AMAZON_DIR\n",
    "]\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, size=None):\n",
    "        self.data = data\n",
    "        self.size = size if size is not None else len(self.data[list(self.data.keys())[0]])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "def count_training_samples(folders, datasets):\n",
    "    total_preds = 0\n",
    "    for folder_name in folders:\n",
    "        for dataset_name in datasets:\n",
    "            try:\n",
    "                train, _, _ = config.load_datasets(folder_name, dataset_name)\n",
    "                total_preds += len(train)\n",
    "            except:\n",
    "                print(f\"Dataset {folder_name}_{dataset_name} does not exist\")\n",
    "                continue\n",
    "    print(f\"Total training samples: {total_preds}\\n\")\n",
    "    return total_preds\n",
    "\n",
    "def create_dataset_dict(tableA_df, tableB_df, ltable_id, rtable_id, label):\n",
    "    return {\n",
    "        \"tableA_df\": tableA_df, \n",
    "        \"tableB_df\": tableB_df, \n",
    "        \"ltable_id\": ltable_id, \n",
    "        \"rtable_id\": rtable_id, \n",
    "        \"label\": label\n",
    "    }\n",
    "\n",
    "def load_and_prepare_datasets(folders, datasets):\n",
    "    all_datasets = {}\n",
    "    for folder_name in folders:\n",
    "        for dataset_name in datasets:\n",
    "            try:\n",
    "                train, val, test = config.load_datasets(folder_name, dataset_name)\n",
    "                tableA_df, tableB_df = config.tableA_tableB(folder_name, dataset_name)\n",
    "                the_dataset = f\"{folder_name}_{dataset_name}\"\n",
    "\n",
    "                all_datasets[f\"{the_dataset}_train\"] = create_dataset_dict(\n",
    "                    tableA_df, tableB_df, train['ltable_id'], train['rtable_id'], train['label']\n",
    "                )\n",
    "                all_datasets[f\"{the_dataset}_val\"] = create_dataset_dict(\n",
    "                    tableA_df, tableB_df, val['ltable_id'], val['rtable_id'], val['label']\n",
    "                )\n",
    "                all_datasets[f\"{the_dataset}_test\"] = create_dataset_dict(\n",
    "                    tableA_df, tableB_df, test['ltable_id'], test['rtable_id'], test['label']\n",
    "                )\n",
    "            except:\n",
    "                print(f\"Dataset {folder_name}_{dataset_name} does not exist\")\n",
    "                continue\n",
    "    return all_datasets\n",
    "\n",
    "def preprocess_function(dataset, tokenizer):\n",
    "    tokenized_inputs = []\n",
    "    labels = []\n",
    "    total_count_0 = sum(label == 0 for label in dataset['label'])\n",
    "    total_count_1 = sum(label == 1 for label in dataset['label'])\n",
    "    count_0, count_1 = 0, 0\n",
    "    for l_id, r_id, label in zip(dataset['ltable_id'], dataset['rtable_id'], dataset['label']):\n",
    "        # If the label is 0 (majority class) and we have already added enough samples of this class, skip this sample\n",
    "        if label == 0 and count_0 >= total_count_1:\n",
    "            continue\n",
    "        entity1 = dataset['tableA_df'].loc[l_id].drop('id')\n",
    "        entity2 = dataset['tableB_df'].loc[r_id].drop('id')\n",
    "        entity1 = ' '.join(f'{col}: {val}' for col, val in entity1.items())\n",
    "        entity2 = ' '.join(f'{col}: {val}' for col, val in entity2.items())\n",
    "        tokenized_inputs.append(tokenizer(entity1, entity2, truncation=True, padding='max_length', max_length=512))\n",
    "        labels.append(torch.tensor(label))\n",
    "        # Update the counts\n",
    "        if label == 0:\n",
    "            count_0 += 1\n",
    "        else:\n",
    "            count_1 += 1\n",
    "    return {\n",
    "        'input_ids': [ti['input_ids'] for ti in tokenized_inputs],\n",
    "        'attention_mask': [ti['attention_mask'] for ti in tokenized_inputs],\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "def load_encoded_datasets(encoded_dir, all_datasets):\n",
    "    loaded_datasets = {}\n",
    "    for dataset_name in all_datasets.keys():\n",
    "        try:\n",
    "            loaded_datasets[dataset_name] = torch.load(os.path.join(encoded_dir, f\"{dataset_name}.pt\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load dataset {dataset_name}\")\n",
    "    return loaded_datasets\n",
    "\n",
    "def combine_datasets(loaded_datasets, suffix):\n",
    "    combined_data = {key: [] for key in loaded_datasets[list(loaded_datasets.keys())[0]].data.keys()}\n",
    "    for dataset_name, dataset in loaded_datasets.items():\n",
    "        if dataset_name.endswith(suffix):\n",
    "            for key in combined_data.keys():\n",
    "                combined_data[key].extend(dataset.data[key])\n",
    "    return CustomDataset(combined_data)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "\n",
    "count_training_samples(folders, datasets)\n",
    "all_datasets = load_and_prepare_datasets(folders, datasets)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Uncomment to preprocess datasets\n",
    "# encoded_datasets = {name: preprocess_function(data, tokenizer) for name, data in all_datasets.items()}\n",
    "# for name, dataset in encoded_datasets.items():\n",
    "#     torch.save(dataset, f\"encoded/{name}.pt\")\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "encoded_dir = 'encoded'\n",
    "loaded_datasets = load_encoded_datasets(encoded_dir, all_datasets)\n",
    "\n",
    "for dataset_name, dataset in loaded_datasets.items():\n",
    "    loaded_datasets[dataset_name] = CustomDataset(dataset)\n",
    "\n",
    "combined_train_dataset = combine_datasets(loaded_datasets, \"_train\")\n",
    "combined_val_dataset = combine_datasets(loaded_datasets, \"_val\")\n",
    "combined_test_dataset = combine_datasets(loaded_datasets, \"_test\")\n",
    "\n",
    "train_size = 1000\n",
    "train_dataset = CustomDataset(combined_train_dataset.data, size=train_size)\n",
    "val_dataset = CustomDataset(combined_val_dataset.data, size=train_size)\n",
    "test_dataset = CustomDataset(combined_test_dataset.data, size=train_size)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    learning_rate=1e-4,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "time_now = time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "output_dir = f\"models/combined_{train_size}_{time_now}\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "test_result = trainer.evaluate(eval_dataset=test_dataset)\n",
    "test_result = pd.DataFrame(test_result, index=[0])\n",
    "print(test_result.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Specify the directory where the model is saved\n",
    "model_dir = 'models/combined_10000_23:22:43'\n",
    "\n",
    "# Load the model\n",
    "model = BertForSequenceClassification.from_pretrained(model_dir, device_map='cpu')\n",
    "tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "trainer = Trainer(model=model, compute_metrics=compute_metrics)\n",
    "small_test_dataset = CustomDataset(combined_test_dataset.data)\n",
    "\n",
    "# Evaluate the model\n",
    "test_result = trainer.evaluate(eval_dataset=small_test_dataset)\n",
    "\n",
    "predictions = trainer.predict(small_test_dataset)\n",
    "probabilities = torch.nn.functional.softmax(torch.from_numpy(predictions.predictions), dim=-1)\n",
    "predicted_classes = torch.argmax(probabilities, dim=-1)\n",
    "print(predicted_classes)\n",
    "\n",
    "\n",
    "# Convert test results to a pandas dataframe\n",
    "test_result = pd.DataFrame(test_result, index=[0])\n",
    "\n",
    "# # Print the test results\n",
    "test_result.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
