{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 75662 \n",
      "\n",
      "Dataset structured_Abt-Buy does not exist.\n",
      "Dataset textual_DBLP-ACM does not exist.\n",
      "Dataset textual_Amazon-Google does not exist.\n",
      "Dataset textual_Walmart-Amazon does not exist.\n",
      "Dataset textual_DBLP-GoogleScholar does not exist.\n",
      "Dataset textual_Fodors-Zagats does not exist.\n",
      "Dataset textual_Beer does not exist.\n",
      "Dataset textual_iTunes-Amazon does not exist.\n",
      "Dataset dirty_Abt-Buy does not exist.\n",
      "Dataset dirty_Amazon-Google does not exist.\n",
      "Dataset dirty_Fodors-Zagats does not exist.\n",
      "Dataset dirty_Beer does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlhelin/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'structured_DBLP_ACM_train.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 147\u001b[0m\n\u001b[1;32m    145\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# Adjust this value based on your needs\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Create a smaller training dataset\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m small_train_dataset \u001b[38;5;241m=\u001b[39m CustomDataset(\u001b[43mloaded_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstructured_DBLP_ACM_train.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdata, size\u001b[38;5;241m=\u001b[39mtrain_size)\n\u001b[1;32m    148\u001b[0m small_val_dataset \u001b[38;5;241m=\u001b[39m CustomDataset(loaded_datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstructured_DBLP_ACM_val.pt\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdata, size\u001b[38;5;241m=\u001b[39mtrain_size)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_recall_fscore_support\n",
      "\u001b[0;31mKeyError\u001b[0m: 'structured_DBLP_ACM_train.pt'"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "import sys, os, time, random\n",
    "sys.path.append(\"..\")\n",
    "import config\n",
    "\n",
    "\n",
    "folders = [config.STRUCTURED_DIR, config.TEXTUAL_DIR, config.DIRTY_DIR]\n",
    "datasets = [config.DBLP_ACM_DIR, config.ABT_BUY_DIR, config.AMAZON_GOOGLE_DIR, \\\n",
    "            config.WALMART_AMAZON_DIR, config.DBLP_GOOGLESCHOLAR_DIR, config.FODORS_ZAGATS_DIR, \\\n",
    "                config.BEER_DIR, config.ITUNES_AMAZON_DIR]\n",
    "\n",
    "total_preds = 0\n",
    "for folder_name in folders:\n",
    "    for dataset_name in datasets:\n",
    "        try:\n",
    "            train, val, test = config.load_datasets(folder_name, dataset_name)\n",
    "            total_preds += len(train)\n",
    "        except:\n",
    "            # print(f\"Dataset {folder_name}_{dataset_name} does not exist.\")\n",
    "            continue\n",
    "print(f\"Total training samples: {total_preds} \\n\")\n",
    "\n",
    "\n",
    "# create the huge disctorionary to store all the data\n",
    "def create_dataset_dict(tableA_df, tableB_df, ltable_id, rtable_id, label):\n",
    "    return {\n",
    "        \"tableA_df\": tableA_df, \n",
    "        \"tableB_df\": tableB_df, \n",
    "        \"ltable_id\" : ltable_id, \n",
    "        \"rtable_id\" : rtable_id, \n",
    "        \"label\" : label\n",
    "    }\n",
    "\n",
    "all_datasets = {}\n",
    "\n",
    "for x, folder_name in enumerate(folders):\n",
    "    for y, dataset_name in enumerate(datasets):\n",
    "        try:\n",
    "            train, val, test = config.load_datasets(folder_name, dataset_name)\n",
    "            tableA_df, tableB_df = config.tableA_tableB(folder_name, dataset_name)\n",
    "            the_dataset = folder_name + \"_\" + dataset_name\n",
    "\n",
    "            all_datasets[the_dataset + \"_train\"] = create_dataset_dict(\n",
    "                tableA_df, tableB_df, train['ltable_id'], train['rtable_id'], train['label']\n",
    "            )\n",
    "            all_datasets[the_dataset + \"_val\"] = create_dataset_dict(\n",
    "                tableA_df, tableB_df, val['ltable_id'], val['rtable_id'], val['label']\n",
    "            )\n",
    "            all_datasets[the_dataset + \"_test\"] = create_dataset_dict(\n",
    "                tableA_df, tableB_df, test['ltable_id'], test['rtable_id'], test['label']\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Dataset {folder_name}_{dataset_name} does not exist.\")\n",
    "            continue\n",
    "\n",
    "# Preprocess the dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def preprocess_function(dataset):\n",
    "    # Initialize lists to store the tokenized text data and labels\n",
    "    tokenized_inputs = []\n",
    "    labels = []\n",
    "    # Iterate over the rows in the ltable_id and rtable_id Series\n",
    "    for l_id, r_id, label in zip(dataset['ltable_id'], dataset['rtable_id'], dataset['label']):\n",
    "        # Map the identifiers to their corresponding text data\n",
    "        entity1 = dataset['tableA_df'].loc[l_id]\n",
    "        entity2 = dataset['tableB_df'].loc[r_id]\n",
    "        # Drop id\n",
    "        entity1 = entity1.drop('id')\n",
    "        entity2 = entity2.drop('id')\n",
    "        # Join the text data into a single string with column names as separators\n",
    "        entity1 = ' '.join(f'{col}: {val}' for col, val in entity1.items())\n",
    "        entity2 = ' '.join(f'{col}: {val}' for col, val in entity2.items())\n",
    "        # Tokenize the text data and add it to the list\n",
    "        tokenized_inputs.append(tokenizer(entity1, entity2, truncation=True, padding='max_length', max_length=512))\n",
    "        # Process the label and add it to the list\n",
    "        labels.append(torch.tensor(label))\n",
    "    # Return the tokenized inputs and the labels\n",
    "    return {'input_ids': [ti['input_ids'] for ti in tokenized_inputs], 'attention_mask': [ti['attention_mask'] for ti in tokenized_inputs], 'labels': labels}\n",
    "\n",
    "# Preprocessing to each dataset that takes 2 minutes\n",
    "\"\"\"\n",
    "encoded_datasets = {}\n",
    "for dataset_name, dataset in all_datasets.items():\n",
    "    encoded_datasets[dataset_name] = preprocess_function(dataset)\n",
    "    torch.save(encoded_datasets[dataset_name], f\"encoded/{dataset_name}.pt\")\n",
    "\"\"\"\n",
    "\n",
    "# List of dataset names\n",
    "loaded_datasets = {}\n",
    "\n",
    "# Load the encoded datasets, 17 seconds instead of 2 minutes\n",
    "for dataset_name, dataset in all_datasets.items():\n",
    "    loaded_datasets[dataset_name] = torch.load(f'encoded/{dataset_name}.pt')\n",
    "    \n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, size=None):\n",
    "        self.data = data\n",
    "        self.size = size if size is not None else len(self.data[list(self.data.keys())[0]])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "# Convert the loaded datasets into Dataset objects\n",
    "for dataset_name, dataset in loaded_datasets.items():\n",
    "    loaded_datasets[dataset_name] = CustomDataset(dataset)\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = BertForSequenceClassification.from_pretrained('distilbert/distilbert-base-uncased', num_labels=2)\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "def combine_datasets(loaded_datasets, suffix):\n",
    "    combined_data = {key: [] for key in loaded_datasets[list(loaded_datasets.keys())[0]].data.keys()}\n",
    "    for dataset_name, dataset in loaded_datasets.items():\n",
    "        if dataset_name.endswith(suffix):\n",
    "            for key in combined_data.keys():\n",
    "                combined_data[key].extend(dataset.data[key])\n",
    "    return CustomDataset(combined_data)\n",
    "\n",
    "# Use the function to combine the training and test datasets\n",
    "# combined_train_dataset = combine_datasets(loaded_datasets, \"_train\")\n",
    "# combined_val_dataset = combine_datasets(loaded_datasets, \"_val\")\n",
    "# combined_test_dataset = combine_datasets(loaded_datasets, \"_test\")\n",
    "\n",
    "# Define a smaller size for the training dataset\n",
    "train_size = 100  # Adjust this value based on your needs\n",
    "# Create a smaller training dataset\n",
    "small_train_dataset = CustomDataset(loaded_datasets['structured_DBLP-ACM_train.pt'].data, size=train_size)\n",
    "small_val_dataset = CustomDataset(loaded_datasets['structured_DBLP-ACM_val.pt'].data, size=train_size)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# format current time as a string with HH:MM:SS\n",
    "time_now = time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "\n",
    "# save the model\n",
    "trainer.save_model(f\"models/train_size_{train_size}_{time_now}\")\n",
    "\n",
    "# Test the model\n",
    "test_result = trainer.evaluate(eval_dataset=loaded_datasets['structured_DBLP-ACM_test.pt'].data)\n",
    "\n",
    "# Print the test results\n",
    "print(f\"Test Results: {test_result}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
