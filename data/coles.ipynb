{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, random, os, time\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "def configure_driver(options):\n",
    "    # Setup driver\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
    "    return driver\n",
    "\n",
    "def get_categories(driver, url):\n",
    "    driver.get(url + \"/browse\")\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    raw_categories = soup.find_all(\"a\", class_=\"coles-targeting-ShopCategoriesShopCategoryStyledCategoryContainer\")\n",
    "    not_needed = [\"/browse/down-down\", \"/on-special\", \\\n",
    "                    \"/browse/bonus-ovenware-credits\", \\\n",
    "                    \"/browse/dairy-eggs-fridge\", \\\n",
    "                    \"/browse/fruit-vegetables\", \\\n",
    "                    \"/browse/meat-seafood\", \\\n",
    "                    \"/browse/tobacco\", \\\n",
    "                    \"/browse/liquor\", \\\n",
    "                    \"/browse/bakery\", \\\n",
    "                    \"/browse/deli\", \\\n",
    "                    \"/browse/drinks\", \\\n",
    "                    \"/browse/frozen\", \\\n",
    "                    \"/browse/baby\", \\\n",
    "                    \"/browse/pet\", \\\n",
    "                    \"/browse/household\", \\\n",
    "                    \"/browse/health-beauty\", \\\n",
    "                    \"/browse/pantry\", \\\n",
    "    ]\n",
    "    # NB pantry needs to be added later\n",
    "    categories = [category for category in raw_categories if category.get(\"href\") not in not_needed]\n",
    "    for category in categories:\n",
    "        print(category.text)\n",
    "    return categories\n",
    "\n",
    "def scrape_products_in_category(driver, category, url, first):\n",
    "    category_link = category.get(\"href\")\n",
    "    \n",
    "    category_link = url + category_link\n",
    "    print(category_link)\n",
    "    driver.get(category_link)\n",
    "    if first == 0:\n",
    "        time.sleep(35)\n",
    "    \n",
    "    while True:\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        products = soup.find_all(\"header\", class_=\"product__header\")\n",
    "        filename = category.text + \".csv\"\n",
    "        # make coles directory if it doesn't exist\n",
    "        if not os.path.exists(\"coles\"):\n",
    "            os.makedirs(\"coles\")\n",
    "        colespath = os.path.join(os.getcwd(), 'coles')\n",
    "        filepath = os.path.join(colespath, filename)\n",
    "        \n",
    "        with open(filepath, \"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            \n",
    "            for product in products:\n",
    "                name = product.find(\"h2\", class_=\"product__title\")\n",
    "                price = product.find(\"span\", class_=\"price__value\")\n",
    "                product_link = product.find(\"a\", class_=\"product__link\")[\"href\"]\n",
    "                product_code = product_link.split(\"-\")[-1]\n",
    "                if name and price:\n",
    "                    name = name.text.strip()\n",
    "                    price = price.text.strip()\n",
    "                    link = url + product_link\n",
    "                    writer.writerow([product_code, name, price, link])\n",
    "                    \n",
    "            pagination = soup.find(\"ul\", class_=\"coles-targeting-PaginationPaginationUl\")\n",
    "            if not pagination:\n",
    "                break\n",
    "            if pagination:\n",
    "                pages = pagination.find_all(\"li\")\n",
    "                last_page = int(pages[-2].text.strip()) if pages else 1\n",
    "            else:\n",
    "                last_page = 1\n",
    "            total_pages = int(pages[-2].text.strip())\n",
    "            start = 94      \n",
    "            for page in range(start, last_page + 1):\n",
    "                next_page_link = f\"{category_link}?page={page}\"\n",
    "                driver.get(next_page_link)\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                products = soup.find_all(\"header\", class_=\"product__header\")\n",
    "                print(f\"{page}/{total_pages}\")\n",
    "                for product in products:\n",
    "                    name = product.find(\"h2\", class_=\"product__title\")\n",
    "                    price = product.find(\"span\", class_=\"price__value\")\n",
    "                    product_link = product.find(\"a\", class_=\"product__link\")[\"href\"]\n",
    "                    product_code = product_link.split(\"-\")[-1]\n",
    "                    if name and price:\n",
    "                        name = name.text.strip()\n",
    "                        price = price.text.strip()\n",
    "                        link = url + product_link\n",
    "                        writer.writerow([product_code, name, price, link])\n",
    "                time.sleep(random.randint(1, 6))     \n",
    "            if page == last_page:\n",
    "                break\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the scraping process.\n",
    "    \"\"\"\n",
    "    url = \"https://www.coles.com.au\"\n",
    "    options = Options()\n",
    "    \n",
    "    options.add_argument(\"--enable-javascript\")\n",
    "    options.add_argument(\"--enable-cookies\")\n",
    "    \n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    \n",
    "    ua = UserAgent()\n",
    "    userAgent = ua.random\n",
    "    options.add_argument(f\"user-agent={userAgent}\")\n",
    "    \n",
    "    driver = configure_driver(options)\n",
    "    \n",
    "    try:\n",
    "        print(\"Here we go...\")\n",
    "        categories = get_categories(driver, url)\n",
    "        \n",
    "        first = 0\n",
    "        for x, category in enumerate(categories):\n",
    "            print(f\"\\nNumber: {x}/{len(categories)}\")\n",
    "            scrape_products_in_category(driver, category, url, first)\n",
    "            time.sleep(random.randint(1, 6))\n",
    "            first += 1\n",
    "                            \n",
    "        print(\"Finished\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
