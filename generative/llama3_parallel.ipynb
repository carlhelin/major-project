{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import llama3\n",
    "\n",
    "def process_row(z, tableA, tableB, label, tableA_df, tableB_df, columns, prompt_techniques, extra_correction, dataset_name, save_folder, csv_name, llama3_model):\n",
    "    # Your code here\n",
    "    \n",
    "    idA, idB, single_label = tableA.iloc[z], tableB.iloc[z], label.iloc[z]\n",
    "    rowA = tableA_df[tableA_df['id'] == idA].drop(columns='id')\n",
    "    rowB = tableB_df[tableB_df['id'] == idB].drop(columns='id')\n",
    "    sentenceA = llama3.format_columns_string(*columns).format(**rowA.to_dict('records')[0])\n",
    "    sentenceB = llama3.format_columns_string(*columns).format(**rowB.to_dict('records')[0])\n",
    "\n",
    "    for prompt in prompt_techniques:\n",
    "        for force in extra_correction:\n",
    "            domain = llama3.determine_domain(dataset_name) if prompt not in [llama3.general_simple, llama3.general_complex] else None\n",
    "            prompt_sentence = llama3.generate_prompt_sentence(sentenceA, sentenceB, force, prompt, domain)\n",
    "\n",
    "            start = time.time()\n",
    "            print(prompt_sentence)\n",
    "            response = llama3_model.llama_chat_get_response(prompt_sentence)\n",
    "            print(response)\n",
    "            end = time.time()\n",
    "            time_taken = end - start\n",
    "\n",
    "            pred = llama3.parse_response(response)\n",
    "            simple_or_complex = llama3.determine_complexity(prompt)\n",
    "            general_or_domain = 'domain' if prompt in [llama3.domain_simple, llama3.domain_complex] else 'general'\n",
    "            yes_or_no = 1 if force else 0\n",
    "\n",
    "            # llama3.save_predictions(f\"{save_folder}/{csv_name}.csv\", general_or_domain, simple_or_complex, yes_or_no, idA, idB, pred, single_label, time_taken)\n",
    "\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import config\n",
    "    \n",
    "# Different prompt-techniques\n",
    "prompt_techniques = [llama3.general_simple, llama3.general_complex, llama3.domain_simple, llama3.domain_complex]\n",
    "\n",
    "# 1 for force yes or no response, 0 for not\n",
    "extra_correction = [1]\n",
    "\n",
    "# Different folder and datasets\n",
    "folders = [config.STRUCTURED_DIR, config.DIRTY_DIR]\n",
    "datasets = [config.DBLP_GOOGLESCHOLAR_DIR]\n",
    "\n",
    "# NOT DONE: config.DBLP_GOOGLESCHOLAR_DIR\n",
    "\n",
    "done_dir = [config.AMAZON_GOOGLE_DIR, config.BEER_DIR, config.FODORS_ZAGATS_DIR, \n",
    "            config.ITUNES_AMAZON_DIR, config.WALMART_AMAZON_DIR, config.ABT_BUY_DIR, \n",
    "            config.DBLP_ACM_DIR ]\n",
    "\n",
    "save_folder = 'llama3_predictions'\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "\n",
    "total_preds = 0\n",
    "for folder_name in folders:\n",
    "    for dataset_name in datasets:\n",
    "        try:\n",
    "            train, val, test = config.load_datasets(folder_name, dataset_name)\n",
    "            total_preds += len(test)*4\n",
    "        except:\n",
    "            print(f\"Dataset {folder_name}_{dataset_name} does not exist.\")\n",
    "            continue\n",
    "print(f\"Total predictions: {total_preds}\")\n",
    "\n",
    "llama3_model = llama3.LLama3()\n",
    "for x, folder_name in enumerate(folders):\n",
    "    print(f\"Starting: {folder_name}...\")\n",
    "    for y, dataset_name in enumerate(datasets):\n",
    "        try:\n",
    "            print(f\"    Processing {dataset_name}...\") \n",
    "            csv_name = f\"{folder_name}_{dataset_name}\"\n",
    "\n",
    "            # if not os.path.exists(f\"{save_folder}/{csv_name}.csv\"):\n",
    "            #     with open(f\"{save_folder}/{csv_name}.csv\", 'w') as f:\n",
    "            #         f.write(\"general_or_domain,simple_or_complex,force_or_not,tableA_id,tableB_id,pred,label,time\\n\")\n",
    "\n",
    "            train, val, test = config.load_datasets(folder_name, dataset_name)\n",
    "            tableA_df, tableB_df = config.tableA_tableB(folder_name, dataset_name)\n",
    "\n",
    "            columns = tableA_df.columns\n",
    "            if 'id' in columns:\n",
    "                columns = columns.drop('id')\n",
    "\n",
    "            tableA, tableB, label = test['ltable_id'], test['rtable_id'], test['label']\n",
    "\n",
    "            for z in range(len(tableA)):\n",
    "                process_row(z, tableA, tableB, label, tableA_df, tableB_df, columns, prompt_techniques, extra_correction, dataset_name, save_folder, csv_name, llama3_model)\n",
    "                \n",
    "        except:\n",
    "            print(f\"Dataset {folder_name}_{dataset_name} does not exist.\")\n",
    "            continue\n",
    "    \n",
    "    # 5743 * 4 = 22972 for dirty      DBLP_GOOGLE_SCHOLAR\n",
    "    # 5743 * 4 = 22972 for structured DBLP_GOOGLE_SCHOLAR\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
